{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\syeds\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\syeds\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    " \n",
    "#download the treebank corpus from nltk\n",
    "nltk.download('treebank')\n",
    " \n",
    "#download the universal tagset from nltk\n",
    "nltk.download('universal_tagset')\n",
    " \n",
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    " \n",
    "#print the first two sentences along with tags\n",
    "print(nltk_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80310\n",
      "20366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "# create list of train and test tagged words\n",
    "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
    "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.83371304e-03 1.41230067e-02 4.19134386e-02 5.01138950e-03\n",
      "  2.12756261e-01 3.69020514e-02 9.56719834e-03 4.84738052e-01\n",
      "  6.83371304e-03 2.23234631e-02 8.83826911e-02 7.06150308e-02]\n",
      " [1.76125243e-02 1.17416831e-03 4.50097844e-02 2.34833662e-03\n",
      "  2.50489235e-01 9.39334650e-03 1.01369865e-01 4.01174158e-01\n",
      "  5.67514673e-02 1.95694715e-02 1.21330721e-02 8.29745606e-02]\n",
      " [6.87694475e-02 2.78940029e-03 9.23720598e-02 6.00793920e-02\n",
      "  2.18538776e-01 5.25694676e-02 1.72191828e-01 8.96899477e-02\n",
      "  7.82104954e-02 9.29084867e-02 2.56410260e-02 4.61323895e-02]\n",
      " [6.03732169e-02 4.39077942e-03 3.51262353e-02 5.48847427e-04\n",
      "  3.49066973e-01 5.70801310e-02 1.23490669e-01 1.50384188e-01\n",
      "  4.06147093e-02 5.59824370e-02 9.33040585e-03 1.13611415e-01]\n",
      " [4.65906132e-03 4.39345129e-02 2.40094051e-01 4.24540639e-02\n",
      "  2.62344331e-01 1.68945398e-02 1.31063312e-02 1.49133503e-01\n",
      "  9.14395228e-03 1.76826611e-01 2.88252197e-02 1.25838192e-02]\n",
      " [1.20248254e-02 1.47401085e-02 1.39255241e-01 6.98215654e-03\n",
      "  3.21955010e-02 8.14584941e-02 7.13731572e-02 3.39022487e-01\n",
      "  2.98681147e-02 1.19472459e-01 2.28859577e-02 1.30721495e-01]\n",
      " [3.30602261e-03 2.87480245e-04 1.73925534e-02 4.31220367e-04\n",
      "  6.35906279e-01 1.20741697e-02 6.03708485e-03 4.02472317e-02\n",
      "  2.28546783e-02 9.91806854e-03 4.51343954e-02 2.06410810e-01]\n",
      " [3.55432779e-02 3.06629837e-02 3.48066315e-02 5.43278083e-03\n",
      "  1.10589318e-01 8.38858187e-02 1.33609578e-01 1.67955801e-01\n",
      "  2.28360966e-02 9.23572779e-02 2.15930015e-01 6.63904250e-02]\n",
      " [1.42806140e-03 2.60621198e-02 1.19243130e-01 1.42806144e-02\n",
      "  3.51660132e-01 3.57015361e-03 3.57015361e-03 2.07068902e-02\n",
      "  1.84219927e-01 3.74866128e-02 2.02427700e-01 3.53445187e-02]\n",
      " [6.96026310e-02 1.26550242e-03 3.87243740e-02 1.01240189e-03\n",
      "  3.23588967e-01 1.45532778e-02 3.20931405e-01 8.47886596e-03\n",
      "  6.32751212e-02 1.69577319e-02 3.45482156e-02 1.07061505e-01]\n",
      " [5.41995019e-02 1.85085520e-01 1.60868734e-01 1.03786280e-02\n",
      "  6.16951771e-02 2.57543717e-02 5.68902567e-02 2.06419379e-01\n",
      "  3.07514891e-03 1.42225638e-01 7.57255405e-02 1.76821072e-02]\n",
      " [1.94174761e-04 1.14563107e-02 6.60194159e-02 1.68932043e-02\n",
      "  6.96893215e-01 5.24271838e-03 5.24271838e-03 1.14563107e-02\n",
      "  2.17475723e-02 8.05825219e-02 2.09708735e-02 6.33009672e-02]]\n"
     ]
    }
   ],
   "source": [
    "# compute Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)#total number of times the passed tag occurred in train_bag\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "#now calculate the total number of times the passed word occurred as the passed tag.\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    " \n",
    "     \n",
    "    return (count_w_given_tag, count_tag)\n",
    "\n",
    "# compute  Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)\n",
    "\n",
    "# creating t x t transition matrix of tags, t= no of tags\n",
    "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
    " \n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Will', 'PRON'), ('can', 'VERB'), ('see', 'VERB'), ('Marry', 'PRON')]\n"
     ]
    }
   ],
   "source": [
    "test_sent=\"Will can see Marry\"\n",
    "#pred_tags_rule=Viterbi_rule_based(test_sent.split())\n",
    "pred_tags_withoutRules= Viterbi(test_sent.split())\n",
    "#print(pred_tags_rule)\n",
    "print(pred_tags_withoutRules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_withoutRules= Viterbi(test_sent.split())\n",
    "#print(pred_tags_rule)\n",
    "print(pred_tags_withoutRules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
